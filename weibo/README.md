# å¾®åšæ•°æ®BERTopicä¸»é¢˜å»ºæ¨¡é¡¹ç›®

## ğŸ“– é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®ä½¿ç”¨BERTopicå¯¹å¾®åšæ•°æ®è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼ŒåŒ…å«å®Œæ•´çš„æ•°æ®é¢„å¤„ç†ã€åµŒå…¥ç”Ÿæˆã€ä¸»é¢˜å»ºæ¨¡å’Œè¯„ä¼°æµç¨‹ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€é”®è¿è¡Œå®Œæ•´æµç¨‹
```bash
# 1. ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨
ls data/data_new.xlsx

# 2. è¿è¡Œå®Œæ•´æµç¨‹
python 00_run_pipeline.py
```

### ç¯å¢ƒè®¾ç½®
```bash
# è‡ªåŠ¨è®¾ç½®ç¯å¢ƒ
python 0_setup_environment.py

# æˆ–ä½¿ç”¨conda
conda env create -f requirements_minimal.yml
conda activate weibo-bertopic-minimal
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
weibo/
â”œâ”€â”€ 00_run_pipeline.py              # å®Œæ•´æµç¨‹è¿è¡Œè„šæœ¬
â”œâ”€â”€ 0_setup_environment.py          # ç¯å¢ƒè®¾ç½®è„šæœ¬
â”œâ”€â”€ 1_data_preprocessing.py         # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”œâ”€â”€ 2_analyze_vocabulary.py         # è¯æ±‡åˆ†æè„šæœ¬
â”œâ”€â”€ 3_word_segmentation.py          # åˆ†è¯å¤„ç†è„šæœ¬
â”œâ”€â”€ 4_generate_embeddings.py        # åµŒå…¥å‘é‡ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ 5_main_weibo.py                 # ä¸»é¢˜å»ºæ¨¡ä¸»ç¨‹åº
â”œâ”€â”€ 6_evaluation_metrics.py         # è¯„ä¼°æŒ‡æ ‡è®¡ç®—è„šæœ¬
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ data_new.xlsx              # åŸå§‹å¾®åšæ•°æ®
â”‚   â”œâ”€â”€ æ–‡æœ¬.txt                   # æ¸…ç†åçš„æ–‡æœ¬ï¼ˆ1_data_preprocessing.pyäº§å‡ºï¼‰
â”‚   â”œâ”€â”€ æ—¶é—´.txt                   # æ—¶é—´ä¿¡æ¯ï¼ˆ1_data_preprocessing.pyäº§å‡ºï¼‰
â”‚   â”œâ”€â”€ weibo_clean_data.csv       # æ¸…ç†åçš„å®Œæ•´æ•°æ®ï¼ˆ1_data_preprocessing.pyäº§å‡ºï¼‰
â”‚   â”œâ”€â”€ åˆ‡è¯.txt                   # åˆ†è¯ç»“æœï¼ˆ3_word_segmentation.pyäº§å‡ºï¼‰
â”‚   â””â”€â”€ embedding_*.npy            # åµŒå…¥å‘é‡æ–‡ä»¶ï¼ˆ4_generate_embeddings.pyäº§å‡ºï¼‰
â”œâ”€â”€ results/                        # ç»“æœè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ weibo_complete_results.csv # å®Œæ•´èšç±»ç»“æœ
â”‚   â”œâ”€â”€ weibo_topic_info.csv       # ä¸»é¢˜è¯¦ç»†ä¿¡æ¯
â”‚   â”œâ”€â”€ weibo_analysis_stats.csv   # åˆ†æç»Ÿè®¡
â”‚   â””â”€â”€ weibo_evaluation_metrics.csv # è¯„ä¼°æŒ‡æ ‡
â”œâ”€â”€ vocabulary_analysis/            # è¯æ±‡åˆ†æç»“æœ
â”‚   â””â”€â”€ wordcloud.png              # è¯æ±‡äº‘å›¾
â”œâ”€â”€ embedding/                      # åµŒå…¥å‘é‡æ–‡ä»¶
â”œâ”€â”€ åˆ†è¯/                          # åˆ†è¯ç›¸å…³æ–‡ä»¶
â”‚   â”œâ”€â”€ stopwords.txt              # åœç”¨è¯è¡¨
â”‚   â”œâ”€â”€ userdict.txt               # ç”¨æˆ·è¯å…¸
â”‚   â””â”€â”€ userdict_updated.txt       # æ›´æ–°åçš„ç”¨æˆ·è¯å…¸
â”œâ”€â”€ requirements.yml               # å®Œæ•´ç¯å¢ƒé…ç½®
â”œâ”€â”€ requirements_minimal.yml       # æœ€å°ç¯å¢ƒé…ç½®
â””â”€â”€ README.md                      # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸ”„ å¤„ç†æµç¨‹

### å®Œæ•´æµç¨‹æ­¥éª¤
1. **æ•°æ®é¢„å¤„ç†** (`1_data_preprocessing.py`) - æ¸…ç†Excelæ•°æ®ï¼Œæå–æ–‡æœ¬å’Œæ—¶é—´ä¿¡æ¯
   - äº§å‡ºï¼š`data/æ–‡æœ¬.txt`, `data/æ—¶é—´.txt`, `data/weibo_clean_data.csv`
2. **è¯æ±‡åˆ†æ** (`2_analyze_vocabulary.py`) - ç»Ÿè®¡é«˜é¢‘è¯æ±‡ï¼Œç”Ÿæˆç”¨æˆ·è‡ªå®šä¹‰è¯å…¸å»ºè®®
   - äº§å‡ºï¼š`vocabulary_analysis/wordcloud.png`, `åˆ†è¯/userdict_updated.txt`
3. **åˆ†è¯å¤„ç†** (`3_word_segmentation.py`) - ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
   - äº§å‡ºï¼š`data/åˆ‡è¯.txt`, `data/åˆ‡è¯_word_freq.csv`
4. **åµŒå…¥å‘é‡ç”Ÿæˆ** (`4_generate_embeddings.py`) - ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡
   - äº§å‡ºï¼š`data/embedding_sen.npy`, `data/embedding_bert.npy`, `data/embedding_info.csv`
5. **ä¸»é¢˜å»ºæ¨¡** (`5_main_weibo.py`) - ä½¿ç”¨BERTopicè¿›è¡Œä¸»é¢˜å»ºæ¨¡
   - äº§å‡ºï¼š`results/weibo_*.csv` ç³»åˆ—æ–‡ä»¶
6. **è¯„ä¼°åˆ†æ** (`6_evaluation_metrics.py`) - è®¡ç®—ä¸»é¢˜å»ºæ¨¡è¯„ä¼°æŒ‡æ ‡
   - äº§å‡ºï¼š`results/weibo_evaluation_metrics.csv`

### å•ç‹¬è¿è¡Œæ­¥éª¤
```bash
# ç¯å¢ƒè®¾ç½®
python 0_setup_environment.py

# æ•°æ®é¢„å¤„ç†
python 1_data_preprocessing.py

# è¯æ±‡åˆ†æ
python 2_analyze_vocabulary.py

# åˆ†è¯å¤„ç†
python 3_word_segmentation.py

# åµŒå…¥å‘é‡ç”Ÿæˆ
python 4_generate_embeddings.py

# ä¸»é¢˜å»ºæ¨¡
python 5_main_weibo.py

# è¯„ä¼°åˆ†æ
python 6_evaluation_metrics.py
```

## ğŸ“Š è¾“å…¥è¾“å‡ºæ–‡ä»¶

### è¾“å…¥æ–‡ä»¶
- `data/data_new.xlsx` - åŸå§‹å¾®åšæ•°æ®
  - åº”åŒ…å«åˆ—ï¼š`txt`(æ–‡æœ¬), `date`(æ—¶é—´), `user`(ç”¨æˆ·), `repost`(è½¬å‘), `comment`(è¯„è®º), `like`(ç‚¹èµ), `type`(ç±»å‹)

### ä¸»è¦è¾“å‡ºæ–‡ä»¶
- `results/weibo_complete_results.csv` - å®Œæ•´èšç±»ç»“æœï¼ˆåŒ…å«åŸæ–‡ã€æ—¶é—´ã€ä¸»é¢˜æ ‡ç­¾ï¼‰
- `results/weibo_topic_info.csv` - ä¸»é¢˜è¯¦ç»†ä¿¡æ¯ï¼ˆå…³é”®è¯ã€ä»£è¡¨æ€§æ–‡æ¡£ï¼‰
- `results/weibo_analysis_stats.csv` - åˆ†æç»Ÿè®¡ï¼ˆæ–‡æ¡£æ•°ã€ä¸»é¢˜æ•°ã€è¦†ç›–ç‡ï¼‰
- `results/weibo_evaluation_metrics.csv` - è¯„ä¼°æŒ‡æ ‡ï¼ˆä¸€è‡´æ€§ã€å¤šæ ·æ€§ã€èšç±»è´¨é‡ï¼‰

### è¯æ±‡åˆ†æç»“æœ
- `vocabulary_analysis/wordcloud.png` - è¯æ±‡äº‘å›¾
- `åˆ†è¯/userdict_updated.txt` - æ›´æ–°åçš„ç”¨æˆ·è¯å…¸

### ä¸­é—´æ–‡ä»¶
- `data/æ–‡æœ¬.txt` - æ¸…ç†åçš„æ–‡æœ¬ï¼ˆ1_data_preprocessing.pyäº§å‡ºï¼‰
- `data/æ—¶é—´.txt` - æ—¶é—´ä¿¡æ¯ï¼ˆ1_data_preprocessing.pyäº§å‡ºï¼‰
- `data/weibo_clean_data.csv` - æ¸…ç†åçš„å®Œæ•´æ•°æ®ï¼ˆ1_data_preprocessing.pyäº§å‡ºï¼‰
- `data/åˆ‡è¯.txt` - åˆ†è¯ç»“æœï¼ˆ3_word_segmentation.pyäº§å‡ºï¼‰
- `data/åˆ‡è¯_word_freq.csv` - è¯é¢‘ç»Ÿè®¡ï¼ˆ3_word_segmentation.pyäº§å‡ºï¼‰
- `data/embedding_*.npy` - åµŒå…¥å‘é‡æ–‡ä»¶ï¼ˆ4_generate_embeddings.pyäº§å‡ºï¼‰
- `data/embedding_info.csv` - åµŒå…¥å‘é‡ä¿¡æ¯ï¼ˆ4_generate_embeddings.pyäº§å‡ºï¼‰

## âš™ï¸ ç¯å¢ƒé…ç½®

### å®Œæ•´ç¯å¢ƒï¼ˆæ¨èï¼‰
```bash
conda env create -f requirements.yml
conda activate BERTopic-Tutorial
```

### æœ€å°ç¯å¢ƒï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
```bash
conda env create -f requirements_minimal.yml
conda activate BERTopic-Tutorial-Minimal
```

## ğŸ”§ å‚æ•°é…ç½®

### èšç±»å‚æ•°è°ƒæ•´
åœ¨`5_main_weibo.py`ä¸­å¯ä»¥è°ƒæ•´ï¼š
```python
# HDBSCANèšç±»å‚æ•°
hdbscan_model = HDBSCAN(
    min_cluster_size=20,  # æœ€å°èšç±»å¤§å°
    min_samples=10,       # æœ€å°æ ·æœ¬æ•°
    metric='euclidean'    # è·ç¦»åº¦é‡
)

# UMAPé™ç»´å‚æ•°
umap_model = UMAP(
    n_neighbors=15,       # é‚»å±…æ•°
    min_dist=0.0,         # æœ€å°è·ç¦»
    metric='cosine'       # è·ç¦»åº¦é‡
)
```

### æ–‡æœ¬å¤„ç†å‚æ•°
åœ¨`3_word_segmentation.py`ä¸­å¯ä»¥è°ƒæ•´ï¼š
```python
def segment_text(text, stopwords, min_length=2):  # æœ€å°è¯é•¿åº¦
```

### åµŒå…¥æ¨¡å‹é€‰æ‹©
åœ¨`4_generate_embeddings.py`ä¸­å¯ä»¥æ›´æ¢æ¨¡å‹ï¼š
```python
# Sentence-Transformersæ¨¡å‹
model_name = 'paraphrase-multilingual-MiniLM-L12-v2'

# BERTæ¨¡å‹
model_name = 'bert-base-chinese'
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–
- å‡å°æ‰¹å¤„ç†å¤§å°ï¼š`batch_size = 32`
- ä½¿ç”¨æ›´å°çš„åµŒå…¥æ¨¡å‹
- åˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†

### è®¡ç®—ä¼˜åŒ–
- ä½¿ç”¨GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
- è°ƒæ•´UMAPçš„`low_memory=True`å‚æ•°
- ä½¿ç”¨æ›´è½»é‡çº§çš„åµŒå…¥æ¨¡å‹

### å¹¶è¡Œå¤„ç†
- è°ƒæ•´æ‰¹å¤„ç†å¤§å°
- ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
- ä¼˜åŒ–å†…å­˜ä½¿ç”¨

## ğŸ› å¸¸è§é—®é¢˜

### 1. å†…å­˜ä¸è¶³
- å‡å°‘æ‰¹å¤„ç†å¤§å°
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹
- å¢åŠ ç³»ç»Ÿå†…å­˜

### 2. ä¾èµ–å†²çª
- ä½¿ç”¨condaç¯å¢ƒéš”ç¦»
- æ£€æŸ¥åŒ…ç‰ˆæœ¬å…¼å®¹æ€§
- é‡æ–°å®‰è£…å†²çªçš„åŒ…

### 3. ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
- å®‰è£…ä¸­æ–‡å­—ä½“
- è®¾ç½®matplotlibå­—ä½“
- ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“

### 4. æ¨¡å‹ä¸‹è½½æ…¢
- ä½¿ç”¨å›½å†…é•œåƒæº
- æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶
- ä½¿ç”¨ä»£ç†

### 5. èšç±»æ•ˆæœä¸ä½³
- è°ƒæ•´`min_cluster_size`å‚æ•°
- å¢åŠ `min_samples`å€¼
- å°è¯•ä¸åŒçš„è·ç¦»åº¦é‡

### 6. åˆ†è¯æ•ˆæœä¸ç†æƒ³
- æ›´æ–°ç”¨æˆ·è‡ªå®šä¹‰è¯å…¸
- è°ƒæ•´åœç”¨è¯åˆ—è¡¨
- æ£€æŸ¥æ–‡æœ¬é¢„å¤„ç†è´¨é‡

## ğŸ’¡ ä½¿ç”¨å»ºè®®

1. **é¦–æ¬¡è¿è¡Œ**ï¼šå»ºè®®å…ˆè¿è¡Œè¯æ±‡åˆ†æï¼ŒæŸ¥çœ‹å¹¶è°ƒæ•´ç”¨æˆ·è¯å…¸
2. **å¤§æ•°æ®é›†**ï¼šåµŒå…¥å‘é‡ç”Ÿæˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
3. **å†…å­˜ä¼˜åŒ–**ï¼šå¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥è°ƒæ•´æ‰¹å¤„ç†å¤§å°
4. **ç»“æœåˆ†æ**ï¼šæŸ¥çœ‹ä¸»é¢˜ä¿¡æ¯ï¼Œæ ¹æ®éœ€è¦è°ƒæ•´BERTopicå‚æ•°
5. **è¯æ±‡ä¼˜åŒ–**ï¼šæ ¹æ®è¯æ±‡åˆ†æç»“æœï¼Œä¼˜åŒ–ç”¨æˆ·è‡ªå®šä¹‰è¯å…¸

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. Pythonç‰ˆæœ¬ï¼ˆæ¨è3.10ï¼‰
2. ä¾èµ–åŒ…ç‰ˆæœ¬å…¼å®¹æ€§
3. ç³»ç»Ÿå†…å­˜å’Œå­˜å‚¨ç©ºé—´
4. ç½‘ç»œè¿æ¥ï¼ˆæ¨¡å‹ä¸‹è½½ï¼‰
5. æ•°æ®æ–‡ä»¶æ ¼å¼å’Œå†…å®¹

## ğŸ“ æ›´æ–°æ—¥å¿—

- v1.0.0: åˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«å®Œæ•´æµç¨‹
- v1.1.0: æ·»åŠ è¯æ±‡åˆ†æåŠŸèƒ½
- v1.2.0: ä¼˜åŒ–ç¯å¢ƒé…ç½®å’Œé”™è¯¯å¤„ç†
- v1.3.0: é‡æ–°ç»„ç»‡è„šæœ¬ç»“æ„ï¼ŒæŒ‰æ‰§è¡Œé¡ºåºç¼–å·
- v1.4.0: å®Œå–„æ–‡æ¡£å’Œè¯„ä¼°æŒ‡æ ‡

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚ 